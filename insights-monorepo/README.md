# Motor de Insights de Reuniones

Un backend ligero que expone endpoints para extracciÃ³n de insights de reuniones usando LLMs locales y en la nube.

## ğŸš€ CaracterÃ­sticas

- **API REST** con endpoints `/llm/extract` y `/intake/manual`
- **LLM local** con vLLM + Qwen2.5-7B-Instruct (fallback a Gemini)
- **ValidaciÃ³n JSON** con schemas versionados
- **Sistema de prompts** con agentes, versiones y experimentos A/B
- **IntegraciÃ³n Supabase** para persistencia
- **Slack Block Kit** para notificaciones
- **Monorepo** con workspaces organizados

## ğŸ“ Estructura

```
insights-monorepo/
â”œâ”€ apps/api/              # API Express
â”œâ”€ packages/
â”‚  â”œâ”€ core/              # Ensamblador de prompts + routing
â”‚  â”œâ”€ llm/               # Proveedores LLM (vLLM, Gemini)
â”‚  â”œâ”€ schemas/           # ValidaciÃ³n JSON con AJV
â”‚  â””â”€ slack/             # Plantillas Block Kit
â”œâ”€ seed/                 # SQL seeds para Supabase
â””â”€ docker-compose.yml    # vLLM local
```

## ğŸ› ï¸ InstalaciÃ³n

### 1. Dependencias

```bash
# Instalar dependencias del monorepo
npm install

# Instalar dependencias de cada workspace
cd apps/api && npm install && cd ../../
cd packages/schemas && npm install && cd ../../
cd packages/llm && npm install && cd ../../
cd packages/core && npm install && cd ../../
cd packages/slack && npm install && cd ../../
```

### 2. Variables de entorno

```bash
cp env.example .env
# Editar .env con tus credenciales
```

### 3. vLLM local (opcional)

```bash
docker compose up -d
# Expone API OpenAI-compatible en http://localhost:8000/v1
```

### 4. Base de datos

```bash
# Ejecutar seeds en Supabase
psql "$SUPABASE_DB_URL" -f seed/tables.sql
psql "$SUPABASE_DB_URL" -f seed/prompts.sql
```

## ğŸš€ Uso

### Desarrollo

```bash
npm run dev
# API disponible en http://localhost:4000
```

### Endpoints

#### Ingesta manual
```bash
curl -X POST http://localhost:4000/intake/manual \
  -H 'Content-Type: application/json' \
  -d '{
    "title": "Demo con ACME",
    "raw_text": "[TRANSCRIPCIÃ“N DE PRUEBA]",
    "team": "ventas",
    "session_type": "discovery",
    "created_by_email": "investigador@empresa.com"
  }'
```

#### ExtracciÃ³n de insights
```bash
curl -X POST http://localhost:4000/llm/extract \
  -H 'Content-Type: application/json' \
  -d '{"meeting_id": "<ID_DEVUELTO_ARRIBA>"}'
```

## ğŸ”§ ConfiguraciÃ³n

### Agentes de prompts

El sistema usa agentes versionados almacenados en Supabase:

- **prompt_agents**: DefiniciÃ³n de agentes por team/session_type
- **prompt_versions**: Versiones de prompts con system/instruction/schema
- **prompt_routing_rules**: Reglas de routing por prioridad
- **prompt_experiments**: Experimentos A/B para testing

### Schemas JSON

Los schemas estÃ¡n en `packages/schemas/src/`:
- `ventas.discovery.schema.json`
- `research.usabilidad.schema.json`

### LLM Providers

- **vLLM local**: Qwen2.5-7B-Instruct (puerto 8000)
- **Gemini**: Fallback a Google Gemini 1.5 Flash

## ğŸ”— IntegraciÃ³n con n8n

La API se integra con n8n como orquestador:

1. **Ingesta**: n8n â†’ `/intake/manual`
2. **Procesamiento**: n8n â†’ `/llm/extract`
3. **Notificaciones**: n8n â†’ Slack Block Kit

## ğŸ“Š MÃ©tricas

El sistema registra mÃ©tricas en `prompt_runs`:
- Tokens de entrada/salida
- Tiempo de ejecuciÃ³n
- Tasa de Ã©xito
- Errores

## ğŸ§ª Testing

```bash
# Health check
curl http://localhost:4000/health

# Test completo
npm run test
```

## ğŸ“ˆ PrÃ³ximos pasos

- [ ] Slack modals para interacciÃ³n
- [ ] Experimentos A/B automÃ¡ticos
- [ ] Retrieval con embeddings
- [ ] Schemas adicionales (soporte, validaciÃ³n)
- [ ] Digest semanal automatizado
